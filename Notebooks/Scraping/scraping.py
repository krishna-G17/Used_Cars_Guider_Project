
city_to_city_name = {
    "abilene":"Abilene, TX",
"akron-canton": "Akron-Canton, OH",
"albany, GA":"Albany, GA",
"albany, NY":"Albany, NY",
"albuquerque":"Albuquerque, NM",
"altoona":"Altoona, PA",
"amarillo":"Amarillo, TX",
"ames, IA":"Ames, IA",
"anchorage":"Anchorage, AK",
"annapolis":"Annapolis, MD",
"ann arbor":"Ann Arbor, MI",
"appleton":"Appleton, WI",
"asheville":"Asheville, NC",
"ashtabula":"Ashtabula, OH",
"athens, GA":"Athens, GA",
"athens, OH":"Athens, OH",
"atlanta":"Atlanta, GA",
"auburn":"Auburn, AL",
"augusta":"Augusta, GA",
"austin":"Austin, TX",
"bakersfield":"Bakersfield, CA",
"baltimore":"Baltimore, MD",
"baton rouge":"Baton Rouge, LA",
"battle creek":"Battle Creek, MI",
"beaumont":"Beaumont, TX",
"bellingham":"Bellingham, WA",
"bemidji":"Bemidji, MN",
"bend":"Bend, OR",
"billings":"Billings, MT",
"binghamton":"Binghamton, NY",
"birmingham, AL":"Birmingham, AL",
"bismarck":"Bismarck, ND",
"bloomington, IN":"Bloomington, IN",
"bloomington, IL":"Bloomington, IL",
"boise":"Boise, ID",
"boone":"Boone, NC",
"boston":"Boston, MA",
"boulder":"Boulder, CO",
"bowling green":"Bowling Green, KY",
"bozeman":"Bozeman, MT",
"brainerd":"Brainerd, MN",
"brownsville":"Brownsville, TX",
"brunswick, GA":"Brunswick, GA",
"buffalo":"Buffalo, NY",
"butte":"Butte, MT",
"cape cod":"Cape Cod, MA",
"catskills":"Catskill, NY",
"cedar rapids":"Cedar Rapids, IA",
"central LA":"Central Louisiana Region, LA",
"central MI":"Central Michigan Region, MI",
"central NJ":"Central New Jersey Region, NJ",
"chambana":"Urbana, IL",
"charleston":"Charleston, SC",
"charleston, WV":"Charleston, WV",
"charlotte":"Charlotte, NC",
"charlottesville":"Charlottesville, VA",
"chattanooga":"Chattanooga, TN",
"chautauqua":"Chautauqua, NY",
"chicago":"Chicago, IL",
"chico":"Chico, CA",
"chillicothe":"Chillicothe, OH",
"cincinnati":"Cincinnati, OH",
"clarksville, TN":"Clarksville, TN",
"cleveland":"Cleveland, OH",
"clovis-portales":"Clovis-Portales Area, NM",
"college station":"College Station, TX",
"colo springs":"Colorado Springs, CO",
"columbia, MO":"Columbia, MO",
"columbia":"Columbia, SC",
"columbus, GA":"Columbus, GA",
"columbus, OH":"Columbus, OH",
"cookeville":"Cookeville, TN",
"corpus christi":"Corpus Christi, TX",
"corvallis":"Corvallis, OR",
"cumberland val":"Cumberland Valley Township, PA",
"dallas":"Dallas-Fort Worth Metropolitan Area, TX",
"danville":"Danville, VA",
"daytona beach":"Daytona Beach, FL",
"dayton":"Dayton, OH",
"decatur, IL":"Decatur, IL",
"deep east TX":"Deep East Texas Area, TX",
"delaware":"Delaware, DE",
"del rio":"Del Rio, TX",
"denver":"Denver, CO",
"des moines":"Des Moines, IA",
"detroit metro":"Detroit, MI",
"dothan, AL":"Dothan, AL",
"dubuque":"Dubuque, IA",
"duluth":"Duluth, MN",
"eastern CO":"East Colorado Region, CO",
"eastern CT":"East Connecticut Region, CT",
"eastern KY":"East Kentucky Region, KY",
"eastern montana":"East Montana Region, MT",
"eastern NC":"Eastern Region of North Carolina , NC",
"eastern WV":"Eastern Panhandle Region of West Virginia, WV",
"eastern shore":"Eastern Shore of Maryland, MD",
"east idaho":"Eastern Idaho Region, ID",
"east oregon":"Eastern Oregon Region, OR",
"eau claire":"Eau Claire, WI",
"elko":"Elko, NV",
"elmira":"Elmira, NY",
"el paso":"El Paso, TX",
"erie, PA":"Erie, PA",
"eugene":"Eugene, OR",
"evansville":"Evansville, IN",
"fairbanks":"Fairbanks, AK",
"fargo":"Fargo, ND",
"farmington, NM":"Farmington, NM",
"fayetteville, AR":"Fayetteville, AR",
"fayetteville, NC":"Fayetteville, NC",
"finger lakes":"Finger Lakes, NY",
"flagstaff":"Flagstaff, AZ",
"flint":"Flint, MI",
"the shoals":"Florence-Muscle Shoals Area, AL",
"florence, SC":"Florence, SC",
"florida keys":"Florida Keys, FL",
"fort collins":"Fort Collins, CO",
"fort dodge":"Fort Dodge, IA",
"fort smith":"Fort Smith, AR",
"fort wayne":"Fort Wayne, IN",
"frederick":"Frederick, MD",
"fredericksburg":"Fredericksburg, VA",
"fresno":"Fresno, CA",
"fort myers":"Fort Myers, FL",
"gadsden":"Gadsden, AL",
"gainesville":"Gainesville, FL",
"galveston":"Galveston, TX",
"glens falls":"Glens Falls, NY",
"gold country":"Gold Country Region, CA",
"grand forks":"Grand Forks, ND",
"grand island":"Grand Island, NE",
"grand rapids":"Grand Rapids, MI",
"great falls":"Great Falls, MT",
"green bay":"Green Bay, WI",
"greensboro":"Greensboro, NC",
"greenville":"Greenville, SC",
"gulfport":"Gulfport, MS",
"hanford":"Hanford, CA",
"harrisburg":"Harrisburg, PA",
"harrisonburg":"Harrisonburg, VA",
"hartford":"Hartford, CT",
"hattiesburg":"Hattiesburg, MS",
"hawaii":"Honolulu, HI",
"heartland FL":"Florida Heartland Region, FL",
"helena":"Helena, MT",
"hickory":"Hickory, NC",
"high rockies":"High Rockies Region, CO",
"hilton head":"Hilton Head, SC",
"holland":"Holland, MI",
"houma":"Houma, LA",
"houston":"Houston, TX",
"hudson valley":"Hudson Valley Region, NY",
"humboldt":"Humboldt County, CA",
"huntington":"Huntington, WV",
"huntsville":"Huntsville, AL",
"imperial co":"Imperial County, CA",
"indianapolis":"Indianapolis, IN",
"inland empire":"Inland Empire, CA",
"iowa city":"Iowa City, IA",
"ithaca":"Ithaca, NY",
"jackson, MI":"Jackson, MI",
"jackson, MS":"Jackson, MS",
"jackson, TN":"Jackson, TN",
"jacksonville, FL":"Jacksonville, FL",
"jacksonville, NC":"Jacksonville, NC",
"janesville":"Janesville, WI",
"jersey shore":"Jersey Shore, NJ",
"jonesboro":"Jonesboro, AR",
"joplin":"Joplin, MO",
"kalamazoo":"Kalamazoo, MI",
"kalispell":"Kalispell, MT",
"kansas city":"Kansas City, MO",
"kenai":"Kenai, AK",
"tri-cities, WA":"Tri-Cities, WA",
"kenosha-racine":"Kenosha-Racine, WI",
"killeen-temple":"Killeen-Temple, TX",
"kirksville":"Kirksville, MO",
"klamath falls":"Klamath Falls, OR",
"knoxville":"Knoxville, TN",
"kokomo":"Kokomo, IN",
"la crosse":"La Crosse, WI",
"lafayette":"Lafayette, LA",
"tippecanoe":"Tippecanoe, IN",
"lake charles":"Lake Charles, LA",
"lakeland":"Lakeland, FL",
"lake of ozarks":"Lake of the Ozarks, MO",
"lancaster, PA":"Lancaster, PA",
"lansing":"Lansing, MI",
"laredo":"laredo, TX",
"la salle co":"LaSalle County, IL",
"las cruces":"Las Cruces, NM",
"las vegas":"Las Vegas, NV",
"lawrence":"Lawrence, KS",
"lawton":"Lawton, OK",
"allentown":"Allentown, PA",
"lewiston":"Lewiston, ID",
"lexington":"Lexington, KY",
"lima-findlay":"Lima-Findlay, OH",
"lincoln":"Lincoln, NE",
"little rock":"Little Rock, AR",
"logan":"Logan, UT",
"long island":"Long Island,NY",
"los angeles":"Los Angeles, CA",
"louisville":"Louisville, KY",
"lubbock":"Lubbock, TX",
"lynchburg":"Lynchburg, VA",
"macon":"Macon, GA",
"madison":"Madison, WI",
"maine":"Maine, ME",
"manhattan":"Manhattan, KS",
"mankato":"Mankato, MN",
"mansfield":"Mansfield, OH",
"mason city":"Mason City, IA",
"mattoon":"Mattoon, IL",
"mcallen":"McAllen, TX",
"meadville":"Meadville, PA",
"medford":"Medford, OR",
"memphis":"Memphis, TN",
"mendocino co":"Mendocino County, CA",
"merced":"Merced, CA",
"meridian":"Meridian, MS",
"milwaukee":"Milwaukee, WI",
"minneapolis":"Minneapolis-St Paul Area, MN",
"missoula":"Missoula, MT",
"mobile, AL":"Mobile, AL",
"modesto":"Modesto, CA",
"mohave co":"Mohave County, AZ",
"monroe, LA":"Monroe, LA",
"monroe, MI":"Monroe, MI",
"monterey":"Monterey, CA",
"montgomery":"Montgomery, AL",
"morgantown":"Morgantown, WV",
"moses lake":"Moses Lake, WA",
"muncie":"Muncie, IN",
"muskegon":"Muskegon, MI",
"myrtle beach":"Myrtle Beach, SC",
"nashville":"Nashville, TN",
"new hampshire":"New Hampshire, NH",
"new haven":"New Haven, CT",
"new orleans":"New Orleans, LA",
"blacksburg":"Blacksburg, VA",
"new york":"New York City, NY",
"norfolk":"Norfolk, VA",
"lake city":"Lake City, FL",
"north dakota":"North Dakota Region, ND",
"northeast SD":"Northeast region of South Dakota, SD",
"northern MI":"North region of Michigan, MI",
"northern WV":"North region of West Virginia, WV",
"northern WI":"North region of Wisconsin, WI",
"north jersey":"North Jersey Region, NJ",
"north MS":"North Mississippi Region, MS",
"north platte":"North Platte, NE",
"northwest CT":"North west region of Connecticut, CT",
"northwest GA":"North west region of Georgia, GA",
"northwest KS":"North west region of Kansas, KS",
"northwest OK":"North west region of oklahoma, OK",
"ocala":"Ocala, FL",
"odessa":"Odessa, TX",
"ogden":"Ogden, UT",
"okaloosa":"Okaloosa, FL",
"oklahoma city":"Oklahoma City, OK",
"olympic pen":"Olympic Peninsula, WA",
"omaha":"Omaha, NE",
"oneonta":"Oneonta, NY",
"orange co":"Orange County, CA",
"oregon coast":"Oregon Coast, OR",
"orlando":"Orlando, FL",
"outer banks":"Outer Banks, NC",
"owensboro":"Owensboro, KY",
"palm springs":"Palm Springs, CA",
"panama city, FL":"Panama City, FL",
"parkersburg":"Parkersburg, WV",
"pensacola":"Pensacola, FL",
"peoria":"Peoria, IL",
"philadelphia":"Philadelphia, PA",
"phoenix":"Phoenix, AZ",
"central SD":"Central South Dakota: Missouri River Region, SD",
"pittsburgh":"Pittsburgh, PA",
"plattsburgh":"Plattsburgh, NY",
"poconos":"Poconos, PA",
"port huron":"Port Huron, MI",
"portland":"Portland, OR",
"potsdam-massena":"Potsdam-Canton-Massena, NY",
"prescott":"Prescott, AZ",
"provo":"Provo, UT",
"pueblo":"Pueblo, CO",
"pullman-moscow":"Pullman-Moscow, WA",
"quad cities":"Davenport, IA",
"raleigh":"Raleigh, NC",
"rapid city":"Rapid City,, SD",
"reading":"Reading, PA",
"redding":"Redding, CA",
"reno":"Reno, NV",
"rhode island":"Rhode Island, RI",
"richmond, IN":"Richmond, IN",
"richmond, VA":"Richmond, VA",
"roanoke":"Roanoke, VA",
"rochester, MN":"Rochester, MN",
"rochester, NY":"Rochester, NY",
"rockford":"Rockford, IL",
"roseburg":"Roseburg, OR",
"roswell":"Roswell, NM",
"sacramento":"Sacramento, CA",
"saginaw":"Saginaw, MI",
"salem":"Salem, OR",
"salina":"Salina, KS",
"salt lake":"Salt Lake City, UT",
"san angelo":"San Angelo, TX",
"san antonio":"San Antonio, TX",
"san diego":"San Diego, CA",
"sandusky":"Sandusky, OH",
"san luis obispo":"San Luis Obispo, CA",
"san marcos":"San Marcos, TX",
"santa barbara":"Santa Barbara, CA",
"santa fe":"Santa Fe, NM",
"santa maria":"Santa Maria, CA",
"sarasota":"Sarasota, FL",
"savannah":"Savannah, GA",
"scottsbluff":"Scottsbluff, NE",
"scranton":"Scranton, PA",
"seattle":"Seattle, WA",
"SF bay area":"San Francisco - Bay Area, CA",
"sheboygan, WI":"Sheboygan, WI",
"show low":"Show Low, AZ",
"shreveport":"Shreveport, LA",
"sierra vista":"Sierra Vista, AZ",
"sioux city":"Sioux City, IA",
"sioux falls":"Sioux Falls, SD",
"siskiyou co":"Siskiyou County, CA",
"skagit":"Skagit, WA",
"south bend":"South Bend, IN",
"south coast":"Southern Bristol and Plymouth counties, MA",
"south dakota":"South Dakota, SD",
"southeast AK":"South Eastern Region of Alaska, AK",
"southeast IA":"South Eastern Region of Iowa, IA",
"southeast KS":"South Eastern Region of Kansas, KS",
"southeast MO":"South Eastern Region of Missouri, MO",
"southern IL":"South Region of Illinois, IL",
"southern MD":"South Region of Maryland, MD",
"southern WV":"South Region of West Virginia, WV",
"south florida":"Miami, FL",
"south jersey":"South Region of New Jersey, NJ",
"southwest KS":"South West Region of Kansas, KS",
"southwest MI":"South West Region of Maryland, MI",
"southwest MN":"South Western Region of Minnesota, MN",
"southwest MS":"South Western Region of Mississippi, MS",
"southwest TX":"South Western Region of Texas, TX",
"southwest VA":"South Western Region of Virginia, VA",
"space coast":"Space Coast, FL",
"spokane":"Spokane, WA",
"springfield, IL":"Springfield, IL",
"springfield":"Springfield, MO",
"state college":"State College, PA",
"statesboro":"Statesboro, GA",
"st augustine":"St Augustine, FL",
"st cloud":"St Cloud, MN",
"st george":"St George, UT",
"stillwater":"Stillwater, OK",
"st joseph":"St Joseph, MO",
"st louis":"st louis, MO",
"stockton":"Stockton, CA",
"susanville":"Susanville, CA",
"syracuse":"Syracuse, NY",
"tallahassee":"Tallahassee, FL",
"tampa bay":"Tampa, FL",
"terre haute":"Terre Haute, IN",
"texarkana":"Texarkana, TX",
"texoma":"Texoma, TX",
"the thumb":"The Thumb, MI",
"toledo":"Toledo, OH",
"topeka":"Topeka, KS",
"treasure coast":"Treasure Coast, FL",
"tri-cities, TN":"Tri-Cities Region, TN",
"tucson":"Tucson, AZ",
"tulsa":"Tulsa, OK",
"tuscaloosa":"Tuscaloosa, AL",
"tuscarawas co":"Tuscarawas County, OH",
"twin falls":"Twin Falls, ID",
"twin tiers":"Twin Tiers, NY",
"east TX":"Tyler, TX",
"yoopers":"Upper Peninsula, MI",
"utica":"Utica, NY",
"valdosta":"Valdosta, GA",
"ventura":"Ventura County, CA",
"vermont":"Vermont, VT",
"victoria, TX":"Victoria, TX",
"visalia-tulare":"Visalia-Tulare, CA",
"waco":"Waco, TX",
"washington, DC":"Washington, DC",
"waterloo":"Waterloo, IA",
"watertown":"Watertown, NY",
"wausau":"Wausau, WI",
"wenatchee":"Wenatchee, WA",
"western IL":"Western Illinois Region, IL",
"western KY":"Western Kentucky Region, KY",
"western MD":"Western Maryland Region, MD",
"western mass":"Western Massachusetts Region, MA",
"western slope":"Western Slope, CO",
"west virginia":"West Virginia, WV",
"wichita falls":"Wichita Falls, TX",
"wichita":"wichita, KS",
"williamsport":"Williamsport, PA",
"wilmington, NC":"Wilmington, NC",
"winchester":"Winchester, VA",
"winston-salem":"Winston-Salem,NC",
"worcester":"Worcester, MA",
"wyoming":"Wyoming, WY",
"yakima":"Yakima, WA",
"york, PA":"York, PA",
"youngstown":"Youngstown, OH",
"yuba-sutter":"Yuba-Sutter, CA",
"yuma":"Yuma, AZ",
"zanesville":"Zanesville, OH"
}

state_abbv_to_state_name = {
    "AL":"ALABAMA",
    "AK":"ALASKA",
    "AZ":"ARIZONA",
    "AR":"ARKANSAS",
    "AS":"AMERICAN SAMOA",
    "CA":"CALIFORNIA",
    "CO":"COLORADO",
    "CT":"CONNECTICUT",
    "DE":"DELAWARE",
    "DC":"DISTRICT OF COLUMBIA",
    "FL":"FLORIDA",
    "AL":"GEORGIA",
    "GU":"GUAM",
    "HI":"HAWAII",
    "ID":"IDAHO",
    "AL":"ILLINOIS",
    "IN":"INDIANA",
    "IA":"IOWA",
    "KS":"KANSAS",
    "KY":"KENTUCKY",
	"LA":"LOUISIANA",
	"ME":"MAINE",
	"MD":"MARYLAND",
	"MA":"MASSACHUSETTS",
	"MI":"MICHIGAN",
	"MN":"MINNESOTA",
	"MS":"MISSISSIPPI",
	"MO":"MISSOURI",
	"MT":"MONTANA",
	"NE":"NEBRASKA",
	"NV":"NEVADA",
	"NH":"NEW HAMPSHIRE",
	"NJ":"NEW JERSEY",
	"NM":"NEW MEXICO",
	"NY":"NEW YORK",
	"NC":"NORTH CAROLINA",
	"ND":"NORTH DAKOTA",
	"MP":"NORTHERN MARIANA IS",
	"OH":"OHIO",
	"OK":"OKLAHOMA",
	"OR":"OREGON",
	"PA":"PENNSYLVANIA",
	"PR":"PUERTO RICO",
	"RI":"RHODE ISLAND",
	"SC":"SOUTH CAROLINA",
	"SD":"SOUTH DAKOTA",
	"TN":"TENNESSEE",
	"TX":"TEXAS",
	"UT":"UTAH",
	"VT":"VERMONT",
 	"VA":"VIRGINIA",
	"VI":"VIRGIN ISLANDS",
	"WA":"WASHINGTON",
	"WV":"WEST VIRGINIA",
	"WI":"WISCONSIN",
	"WY":"WYOMING"
}



from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.firefox.options import Options
from selenium.webdriver import Firefox
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.common.exceptions import WebDriverException
from time import sleep
import warnings
import requests
import pandas as pd
from bs4 import BeautifulSoup
import time
import math
import re

from datetime import datetime

import numpy as np
import pyodbc as odbc
from sqlalchemy import create_engine
import sqlalchemy as sa

warnings.filterwarnings('ignore')


# Get the current date
current_date = datetime.now().strftime("%d_%m_%Y")  # Format the date as Day_Month_Year
cd= datetime.now().strftime("%m_%d_%Y") 
# Capture the start time for postings scraping
start_time_scraping_CG_posts = datetime.now()

options = Options()
options.add_argument("--window-size=1920x1080")
options.add_argument("--headless")  # Headless mode
options.add_argument("--verbose")

browser = Firefox(options=options)
browser.set_page_load_timeout(30)

site_urls, site_locations = [], []


URL = f'https://geo.craigslist.org/iso/us'
browser.get(URL)

all_content = browser.find_elements(By.CSS_SELECTOR, 'ul.height6.geo-site-list')
for i in all_content:
    links = i.find_elements(By.TAG_NAME, 'a')
    for link in links:
        site_urls.append(link.get_attribute('href') if link else None)
    
        site_locations.append(link.text)
    

data = []

try:
    for url in site_urls:
        URL_1 = f'{url}search/cta?bundleDuplicates=1&postedToday=1'
        browser.get(URL_1)
        time.sleep(5)  # Wait for the page to load

        try:
            listings_city_element = browser.find_element(By.CLASS_NAME, 'cl-left-group')
            site_name = listings_city_element.text.splitlines()[1]

            total_listings_element = browser.find_element(By.CLASS_NAME, 'cl-page-number')
            total_listings = int(total_listings_element.text.split('of')[-1].replace(',', ''))
            print(f'Total {total_listings} Listings are observed  in {site_name}')
            total_pages = (total_listings // 120) + 1
            print(f'Total {total_pages} Pages observed in {site_name}')

            for page_number in range(1, total_pages + 1):
                URL = f'{URL_1}#search=1~gallery~{page_number}~0'
                browser.get(URL)
                print(URL)
                time.sleep(10)
                all_content = browser.find_elements(By.CSS_SELECTOR, 'li.cl-search-result.cl-search-view-mode-gallery')
                
                for i in all_content:
                    title_element = i.find_element(By.CSS_SELECTOR, 'span.label') if i.find_elements(By.CSS_SELECTOR, 'span.label') else None
                    price_element = i.find_element(By.CLASS_NAME, 'priceinfo') if i.find_elements(By.CLASS_NAME, 'priceinfo') else None
                    tag_element = i.find_element(By.CLASS_NAME, 'meta') if i.find_elements(By.CLASS_NAME, 'meta') else None
                    
                    try:
                        link_element = i.find_element(By.TAG_NAME, 'a')
                        url = link_element.get_attribute('href') if link_element else None
                    except NoSuchElementException:
                        print(f"No link found in the element, skipping: {title_element}")
                    data.append({
                        'City': site_name,
                        'URL': url,
                        'Title': title_element.text if title_element else None,
                        'Price': price_element.text if price_element else None,
                        'Tags': tag_element.text if tag_element else None
                    })
                    
                    
        except Exception as e:
            print(f"An error occurred: {e}")

finally:
    browser.quit()

# Compile the scraped data into a pandas DataFrame
postings_df = pd.DataFrame(data)

# Capture the end time and calculate duration
end_time_scraping_CG_posts = datetime.now()
duration = (end_time_scraping_CG_posts - start_time_scraping_CG_posts).total_seconds()
hours, remainder = divmod(duration, 3600)
minutes, seconds = divmod(remainder, 60)

# Output the timing information
print(f"Posts Scraping completed in: {int(hours)} hours, {int(minutes)} minutes, and {int(seconds)} seconds.")


# Define a function to normalize city names and URLs for comparison
def normalize_text(text):
    # Remove non-alphabetic characters and convert to lowercase
    return re.sub(r'[^a-zA-Z]', '', text).lower()

# Mapping of special cases where city names don't match URL parts directly
special_cases = {
    'birminghamal': 'bham', 
    'cumberlandval': 'chambersburg', 
    'bowlinggreen': 'bgky', 
    'bloomingtonil': 'bn', 
    'clovisportales': 'clovis',
    'deepeasttx':'nacogdoches',
    'easternct':'newlondon', 
    'easternwv':'martinsburg',
        'hawaii':'honolulu', 
            'tricitieswa':'kpr',
     'kenosharacine':'racine',
         'killeentemple':'killeen',           
    'fayettevillear':'fayar',
    'lakeofozarks':'loz',
    'limafindlay':'limaohio',
    'manhattan':'ksu',
    'newhampshire':'nh',
    'northdakota':'nd',
    'northwestks':'nwks',
    'northwestct':'nwct',
    'northwestga':'nwga',
    'northwestok':'enid',
    'potsdammassena':'potsdam',
    'pullmanmoscow':'pullman',
    'rhodeisland':'providence',
    'sanluisobispo':'slo',
    'southdakota':'sd',
    'southeastak':'juneau',
    'southflorida':'miami',
    'southwestmn':'marshall',
    'southwestms':'natchez',
    'southwesttx':'bigbend',
    'yoopers':'up',
    'visaliatulare':'visalia',
    'westernil':'quincy',
    'westvirginia':'wv',
    'heartlandfl':'cfl',
    'highrockies':'rockies',
    'jacksonmi':'jxn',
    'jacksonvillenc':'onslow',
    'northeastsd':'nesd',
    'northernmi':'nmi',
    'northernwv':'wheeling',
    'centralsd':'csd',
    'rochestermn':'rmn',
    'southeastia':'ottumwa',
    'southeastks':'seks',
    'southeastmo':'semo',
    'southernil':'carbondale',
    'southernmd':'smd',
    'southernwv':'swv',
    'southwestks':'swks',
    'southwestmi':'swmi',
    'southwestva':'swva',
    'statecollege':'pennstate',
    'westernky':'westky'
    
    
}


# Define a function to normalize city names and URLs for comparison
def normalize_text(text):
    # Remove non-alphabetic characters and convert to lowercase
    return re.sub(r'[^a-zA-Z]', '', text).lower()

# Define a function that checks if parts of the city in the 'City' column are in the 'URL'
def city_in_url(row):
    # Normalize the city name from the 'City' column
    city_parts = [normalize_text(part) for part in row['City'].split()]
    
    # Normalize the city name from the 'City' column
    city_normalized = normalize_text(row['City'])
    
    # Check if the city has a special mapping and use it if available
    if city_normalized in special_cases:
        city_normalized = special_cases[city_normalized]

    # Extract and normalize the city part from the URL
    url_part = normalize_text(row['URL'].split("//")[1].split(".")[0])

    # Check if any key part of the city name is present in the URL part
    return (any(city_part in url_part for city_part in city_parts) | (city_normalized in url_part))


# Apply the function to each row
# This creates a boolean Series where True indicates a match between City and URL
matches = postings_df.apply(city_in_url, axis=1)

# Create a new DataFrame with rows where the city matches the URL
postings_df_matched_rows = postings_df[matches]

postings_df_matched_rows = postings_df_matched_rows.drop_duplicates().reset_index()

# Use the formatted date in the filename
postings_filename = f'C:/My Files/KG PC Backup/Personal Projects/UCPP/Notebooks/Data/postings/posting_df_{current_date}.csv'

# Save the DataFrame to CSV with the dynamic filename
postings_df_matched_rows.to_csv(postings_filename)

# Ensure that the 'City' column is present in both DataFrames
if 'City' in postings_df.columns and 'City' in postings_df_matched_rows.columns:
    # Find cities in postings_df that are not in matched_postings_df
    unmatched_cities = postings_df[~postings_df['City'].isin(postings_df_matched_rows['City'])]

    # If you just need the unique list of such cities
    unique_unmatched_cities = unmatched_cities['City'].unique()
else:
    print("Error: 'City' column not found in one of the DataFrames")


print("Number of unmatched cities",len(unique_unmatched_cities))

options = Options()
options.add_argument("--window-size=1920x1080")
options.add_argument("--headless")  # Headless mode
options.add_argument("--verbose")

browser = Firefox(options=options)
browser.set_page_load_timeout(30)

# Function to scrape car attributes
def scrape_car_attributes(browser, url):
    car_attributes = {'Posting_URL': url}  # Include URL in the attributes for tracking
    try:
        browser.get(url)
        WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.ID, 'postingbody')))
        posting_content = browser.find_element(By.ID, 'postingbody')
        car_attributes['Post Content'] = posting_content.text
        # Add other scraping logic here
        # Add other scraping logic here
        all_content = browser.find_elements(By.CSS_SELECTOR,'div.mapAndAttrs')
        for i in all_content:
                   
            manuf_year = i.find_element(By.CSS_SELECTOR, 'span.valu.year') if i.find_elements(By.CSS_SELECTOR, 'span.valu.year') else None
            car_attributes['Manufacturing Year'] = manuf_year.text
            

            # Find the manufacturer and model name
            manuf_model_element = i.find_element(By.CSS_SELECTOR, 'a.valu.makemodel') if i.find_elements(By.CSS_SELECTOR, 'a.valu.makemodel') else None
            if manuf_model_element:
                parts = manuf_model_element.text.split(" ")
                parts = [part for part in parts if not part.isnumeric()]  # Remove numeric parts (like years)
                if len(parts) >= 2:
                    # Assuming the first non-numeric part is the manufacturer and the rest is the model
                    car_attributes['Manufacturer'] = parts[0]
                    car_attributes['Model Name'] = " ".join(parts[1:])
                elif len(parts) == 1:
                    # In case there's only one part, we'll use it as the model name
                    # This is a simplification and may not be accurate for all cases
                    car_attributes['Manufacturer'] = None
                    car_attributes['Model Name'] = parts[0]
            else:
                car_attributes['Manufacturer'] = None
                car_attributes['Model Name'] = None
            
            # End of the correction space
            y = i.find_elements(By.CSS_SELECTOR, 'div.attrgroup')
            zm = y[1].text
            lines = zm.splitlines()

            

            for line in lines:
                if not line.strip():
                    continue
                key,value = line.split(":", 1)

                key = key.strip()
                
                #Add the key - value pair to the dictionary
                car_attributes[key] = value.strip()

    except WebDriverException as e:
        # Instead of printing the error, return a code or simple message
        car_attributes['Error'] = 'Page not found or element missing'
    return car_attributes


# Assume df2_without_duplicates is already defined and it's the DataFrame you want to process
posting_attributes = []

# Capture the start time
start_time_each_post_scraping = datetime.now()

# Iterate over the DataFrame rows
for index, row in postings_df_matched_rows.iterrows():
    print(f"Scraping row {index} and URL: {row['URL']}")
    attributes = scrape_car_attributes(browser, row['URL'])
    posting_attributes.append(attributes)

browser.quit()  # Make sure to quit the browser after the loop

# Assign the list of results to the 'Posting_Attributes' column
postings_df_matched_rows['Posting_Attributes'] = posting_attributes

# The rest of your DataFrame manipulation and saving code goes here

# Capture the end time and calculate duration
end_time_each_post_scraping = datetime.now()
duration = (end_time_each_post_scraping - start_time_each_post_scraping).total_seconds()
hours, remainder = divmod(duration, 3600)
minutes, seconds = divmod(remainder, 60)

# Output the timing information
print(f"Each post scraping loop is completed in: {int(hours)} hours, {int(minutes)} minutes, and {int(seconds)} seconds.")


# Normalize the 'Posting_Attributes' column to create a DataFrame where keys of dictionaries become columns.
attributes_df = pd.json_normalize(postings_df_matched_rows['Posting_Attributes'])

# Join the new attributes DataFrame with the original df2 DataFrame.
# This assumes that df2's index aligns with the attributes_df's index.
detailed_postings_df = postings_df_matched_rows.join(attributes_df)

# Replace NaN values with 'N/A' or any other placeholder you prefer.
detailed_postings_df.fillna('N/A', inplace=True)

# Use the formatted date in the filename
scraped_data_filename = f'C:/My Files/KG PC Backup/Personal Projects/UCPP/Notebooks/Data/scraped_data/scraped_data_{current_date}.csv'

# Save the DataFrame to CSV with the dynamic filename
detailed_postings_df.to_csv(scraped_data_filename)

# Create a DataFrame with the timing information
timing_info_df = pd.DataFrame({
    'Date': [current_date],
    'Start Time Scraping Totals posts': [start_time_scraping_CG_posts.strftime('%Y-%m-%d %H:%M:%S')],
    'End Time Scraping Totals posts': [end_time_scraping_CG_posts.strftime('%Y-%m-%d %H:%M:%S')],
    'Start Time Scraping each post': [start_time_each_post_scraping.strftime('%Y-%m-%d %H:%M:%S')],
    'End Time Scraping each post': [end_time_each_post_scraping.strftime('%Y-%m-%d %H:%M:%S')]
})

# Use the formatted date in the filename
time_log_filename = f'C:/My Files/KG PC Backup/Personal Projects/UCPP/Notebooks/Data/scraped_data/time_log/scraping_time_log_{current_date}.csv'

# Save the DataFrame to CSV with the dynamic filename
timing_info_df.to_csv(time_log_filename)

# Convert cd to a datetime object
cd_datetime = pd.to_datetime(cd, format='%m_%d_%Y')

# Assign the datetime object to the 'Date' column of detailed_postings_df
detailed_postings_df['Date'] = cd_datetime


print("Number of rows being added to database table Used_Cars_DF", len(detailed_postings_df[detailed_postings_df['Error'] != 'Page not found or element missing']))


# Function to lookup state
def lookup_city_name(city):
    return city_to_city_name.get(city, None)  # Returns None if the city is not found

# Function to lookup state
def lookup_state_name(state):
    return state_abbv_to_state_name.get(state, None)  # Returns None if the city is not found

# Function to extract state abbreviation from city names
def extract_state_abbreviation(city):
    # Split the city name by comma and strip any leading/trailing whitespace
    parts = city.split(',')
    if len(parts) > 1:
        # Return the state abbreviation, which is the second part after splitting
        return parts[1].strip()
    else:
        # Return None if there's no comma (and thus no state abbreviation)
        return None


detailed_postings_df['City Name'] = detailed_postings_df['City'].apply(lookup_city_name)
# Apply the function to the City column to create a new State column
detailed_postings_df['State'] = detailed_postings_df['City Name'].apply(extract_state_abbreviation)
detailed_postings_df['State Name'] = detailed_postings_df['State'].apply(lookup_state_name)

idead_table_columns_df = detailed_postings_df[detailed_postings_df['Error'] != 'Page not found or element missing'][['City', 'URL', 'Title', 'Price', 'Tags',
       'Posting_Attributes', 'Post Content','Manufacturing Year', 'Manufacturer',
       'Model Name', 'fuel', 'odometer','title status', 'transmission', 'cylinders', 'size',
       'drive', 'type','condition', 'paint color', 'VIN',  'Date', 'State', 'State Name', 'City Name']]


idead_table_columns_df = idead_table_columns_df.fillna('NA')

# Convert 'odometer' column to float, setting non-convertible values to NaN
idead_table_columns_df['odometer'] = pd.to_numeric(idead_table_columns_df['odometer'], errors='coerce')
# Convert 'odometer' column to float, setting non-convertible values to NaN
idead_table_columns_df['Manufacturing Year'] = pd.to_numeric(idead_table_columns_df['Manufacturing Year'], errors='coerce')

idead_table_columns_df['size'] = 'NA'

import json

# Convert the 'Posting_Attributes' column to a JSON string
idead_table_columns_df['Posting_Attributes'] = idead_table_columns_df['Posting_Attributes'].apply(lambda x: json.dumps(x) if isinstance(x, dict) else x)



SERVER = '.'
Database = 'test_UCPP'
Driver = 'ODBC Driver 17 for SQL Server'
Database_conn = f'mssql://@{SERVER}/{Database}?driver={Driver}'

engine = create_engine(Database_conn)
conn = engine.connect()

try:
    # Append data to the database table
    idead_table_columns_df.to_sql('Used_Cars_DF', conn, if_exists='append', index=False)
    
    # Commit changes to the database
    conn.commit()
except TypeError as e: # Assuming you have a variable for index
    print("Error message:", e)
    
    
    
df = pd.read_sql_table('Used_Cars_DF', conn)

print(len(df))

